{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqPcygumMYZ7"
      },
      "source": [
        "This code was inspired by: <br>\n",
        "https://huggingface.co/blog/how-to-train <br>\n",
        "https://huggingface.co/blog/pretraining-bert <br>\n",
        "https://www.kaggle.com/code/arnabs007/pretrain-a-bert-language-model-from-scratch/notebook <br>\n",
        "https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F1TpCVTMYaD"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsiYNcTTMYaE"
      },
      "outputs": [],
      "source": [
        "from datasets import *\n",
        "from transformers import *\n",
        "from tokenizers import *\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHltAUaqczHn"
      },
      "outputs": [],
      "source": [
        "! wandb login [API KEY GOES HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncvZ7GcSMYaH"
      },
      "source": [
        "## Setting the seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGkG9nfZMYaH"
      },
      "outputs": [],
      "source": [
        "seed = 264806\n",
        "torch.manual_seed(seed) # for torch\n",
        "set_seed(seed) # for transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_nuvgRdMYaI"
      },
      "source": [
        "## Train a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PY32crvMYaJ"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_st = bert_tokenizer.all_special_tokens\n",
        "bert_vocab = bert_tokenizer.vocab_size\n",
        "bert_max = bert_tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WKsgg04MYaK"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"\"\"\n",
        "    BERT's tokenizer has:\n",
        "    - Special Tokens -> {bert_st}\n",
        "    - Vocab Size -> {bert_vocab}\n",
        "    - Max length -> {bert_max}\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZIxDF3WMYaL"
      },
      "outputs": [],
      "source": [
        "tokenizer_path = \"BertTokenizer_from_Scratch\"\n",
        "if not os.path.exists(tokenizer_path):\n",
        "    lines = [line.strip() for line in open('FINAL_CORPUS_SEED_264806.txt', 'r', encoding='utf-8').read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "    def batch_iterator(batch_size=10000):\n",
        "        for i in tqdm(range(0, len(lines), batch_size)):\n",
        "            yield lines[i : i + batch_size]\n",
        "\n",
        "    base_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "    new_tokenizer = base_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=bert_vocab)\n",
        "    new_tokenizer.save_pretrained(tokenizer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hnqrIZaMYaM"
      },
      "source": [
        "## Load the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbS7Y6QqMYaN"
      },
      "outputs": [],
      "source": [
        "# adaptation of the LineByLineTextDataset from:\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/language_modeling.py\n",
        "class DownsizedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, tokenizer):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.lines = [line.strip() for line in open(file_path, 'r', encoding='utf-8').read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "        print(f\"Dataset has {len(self.lines)} lines\")\n",
        "        self.examples = []\n",
        "        for line in tqdm(self.lines):\n",
        "            example = tokenizer.encode(line, add_special_tokens=True, truncation=True, max_length=self.tokenizer.model_max_length, padding='max_length')\n",
        "            self.examples.append({'input_ids': torch.tensor(example, dtype=torch.long)})\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wpY2yTBMYaO"
      },
      "outputs": [],
      "source": [
        "tokenizer_path = \"BertTokenizer_from_Scratch\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgdAmYq5MYaP"
      },
      "outputs": [],
      "source": [
        "dataset = DownsizedDataset(\"FINAL_CORPUS_SEED_264806.txt\", tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2glGZEqMYaQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Splitting {len(dataset)} examples into 85-15% train-test\")\n",
        "torch.manual_seed(seed)\n",
        "train_size = int(len(dataset) * 0.85)\n",
        "test_size = len(dataset) - train_size\n",
        "train_split, test_split = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JTbAL2uMYaQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Training has {len(train_split)} examples \\nValidation has {len(test_split)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEELe1f_MYaR"
      },
      "outputs": [],
      "source": [
        "train_split[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXMbTXdgMYaR"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf4BUtllMYaR"
      },
      "outputs": [],
      "source": [
        "def get_last_checkpoint(folder_path):\n",
        "    checkpoints = [f for f in os.listdir(folder_path) if f.startswith(\"checkpoint-\")]\n",
        "\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "\n",
        "    last_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
        "    return os.path.join(folder_path, last_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erT79BJKMYaS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3IWXvuUMYaS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVLic2kRMYaS"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJzLGsbdMYaS"
      },
      "outputs": [],
      "source": [
        "configuration = BertConfig()\n",
        "model = BertForMaskedLM(config=configuration)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tLZGclcMYaT"
      },
      "outputs": [],
      "source": [
        "configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LaMetjqMYaT"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"checkpoints\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=10000,\n",
        "    seed=seed,\n",
        "    report_to='wandb',\n",
        "    logging_strategy='steps',\n",
        "    run_name=\"Downsized BERT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVy15btDMYaT"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=test_split,\n",
        "    train_dataset=train_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D63lKMM8MYaU"
      },
      "outputs": [],
      "source": [
        "if not os.listdir(\"checkpoints\"):\n",
        "    trainer.train()\n",
        "else:\n",
        "    trainer.train(resume_from_checkpoint=get_last_checkpoint(\"checkpoints\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4nySLsbMYaU"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oENsnjPMMYaU"
      },
      "outputs": [],
      "source": [
        "last_checkpoint = get_last_checkpoint(\"checkpoints\")\n",
        "print(\"VALIDATION LOSSES\")\n",
        "with open(f\"{last_checkpoint}/trainer_state.json\", 'r') as f:\n",
        "    trainer_state = json.load(f)\n",
        "    for e in trainer_state['log_history']:\n",
        "        if 'eval_loss' in e:\n",
        "            print(f\"Epoch {e['epoch']} -> {e['eval_loss']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geST_qfAMYaU"
      },
      "outputs": [],
      "source": [
        "model_path = \"Downsized_BERT\"\n",
        "best_checkpoint_model = BertForMaskedLM.from_pretrained('last checkpoint path goes here')\n",
        "best_checkpoint_model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0-AeWIiMYaV"
      },
      "source": [
        "## Test with pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x8JBTMOMYaV"
      },
      "outputs": [],
      "source": [
        "model = BertForMaskedLM.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHbsPgAYMYaV"
      },
      "outputs": [],
      "source": [
        "fill_mask(\"The [MASK] was cloudy yesterday, but today it's rainy.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
